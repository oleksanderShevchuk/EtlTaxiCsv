### **ETL Taxi CSV**

#### ğŸ“„ Overview

This project implements an **ETL pipeline** that imports large New York City Taxi trip datasets (CSV) into a SQL Server database.
It performs validation, deduplication, transformation, and bulk insertion optimized for analytical queries.

#### ğŸ§± Architecture

```
sql/                      # Sql queries
src/
 â”œâ”€â”€ ETL.Core/            # Interfaces, models, and core services
 â”œâ”€â”€ ETL.Infrastructure/  # Implementations (CSV, SQL, I/O, Time)
 â””â”€â”€ ETL.Cli/             # Console app entry point
 â””â”€â”€ ETL.Tests/           # XUnit tests
```

#### âš™ï¸ Technologies

* **.NET 8**
* **Microsoft SQL Server**
* **CsvHelper** for CSV parsing
* **SqlBulkCopy** for efficient bulk loading
* **Serilog** for structured logging
* **Dependency Injection & High-level Architecture** principles

#### ğŸš€ Features

* Reads and transforms trip data from CSV in a streaming fashion
* Detects duplicates based on `(pickup_datetime, dropoff_datetime, passenger_count)`
* Writes duplicates into `duplicates.csv`
* Bulk loads clean records into `dbo.StagingTrips`
* Transforms and filters data into the final `dbo.Trips` table
* Schema optimized for analytical queries

#### ğŸ“Š Database Schema Highlights

* **`Trips`** â€“ main clean dataset
* **`StagingTrips`** â€“ temporary table for raw imports
* **Indexes optimized for:**

  * Average tip per `PULocationId`
  * Top 100 longest trips by distance or duration
  * Search queries filtered by `PULocationId`

#### ğŸ§® Example Analytical Queries

-- You can find in sql folder (perform_queries.sql)

#### ğŸ’¾ Running the Project

1. Configure `appsettings.json`:

```json
{
  "ConnectionStrings": {
    "TaxiDb": "Server=(localdb)\\mssqllocaldb;Database=TaxiTrips;Trusted_Connection=True;TrustServerCertificate=True;"
  },
  "Csv": {
    "BatchSize": 50000,
    "TimeZoneId": "America/New_York",
    "DuplicatesFile": "duplicates.csv"
  }
}
```

2. Run the console app:

```bash
dotnet run --project src/ETL.Cli
```

3. Processed data will be inserted into `TaxiTrips.dbo.Trips`.

## ğŸ³ Run with Docker

### Build and start the environment
```bash
docker-compose up --build

```

#### ğŸ§© Scaling to Large Files

For 10GB+ CSVs:

* Continue using streaming reads and channels.
* Tune `BatchSize` for optimal performance.
* Optionally parallelize bulk inserts.
* Disable and rebuild indexes after bulk load.
